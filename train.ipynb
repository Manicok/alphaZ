{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03b8b8-17a0-4314-8e3d-6da50deecafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from game import Board, Game\n",
    "from MCTS import MCTSPlayer\n",
    "from network import PolicyValueNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b88f70-123f-44a9-93fe-28afd5c82948",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"train_log.txt\",\n",
    "                    format=\"%(asctime)s - %(funcName)s - %(levelname)s: %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f43a74-a434-4885-a2cb-410f5d0c5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, initial_model=None):\n",
    "        self.board_length = 19\n",
    "        self.game = Game(self.board_length)\n",
    "        self.board = self.game.board\n",
    "        self.initial_model = initial_model\n",
    "        self.learning_rate = [1e-2, 1e-3, 1e-4]\n",
    "        self.batch_size = 512\n",
    "        self.game_rounds = 1  # 每一局后都都更新策略\n",
    "        self.check_point = self.game_rounds * 500  # 每500轮检验模型一次\n",
    "        self.train_data = []\n",
    "\n",
    "        if initial_model:\n",
    "            self.PolicyValueNet = PolicyValueNet(self.board_length, initial_model=initial_model)\n",
    "        else:\n",
    "            self.PolicyValueNet = PolicyValueNet(self.board_length)\n",
    "        self.MCTSPlayer = MCTSPlayer(PolicyValueF=self.PolicyValueNet.PolicyValueFunction)\n",
    "\n",
    "    def augment(self, data):\n",
    "        \"\"\"数据增强:棋盘旋转90°、180°或270°，外加镜像共8个不同状态的价值相同\"\"\"\n",
    "        augemnt_data = []\n",
    "        state, prob, winner = data\n",
    "        for i in range(4):\n",
    "            #  旋转\n",
    "            new_state = np.rot90(state, k=i, axes=(1, 2))\n",
    "            new_prob = np.rot90(prob[:-1].reshape(self.board_length, self.board_length),\n",
    "                                k=i).flatten()\n",
    "            new_prob = np.append(new_prob, prob[-1])\n",
    "            augemnt_data.append((new_state, new_prob, winner))\n",
    "            #  镜像\n",
    "            new_state = np.array([np.fliplr(d) for d in state])\n",
    "            new_prob = np.fliplr(prob[:-1].reshape(self.board_length, self.board_length)).flatten()\n",
    "            new_prob = np.append(new_prob, prob[-1])\n",
    "            augemnt_data.append((new_state, new_prob, winner))\n",
    "        return augemnt_data\n",
    "\n",
    "    def collect_data(self):\n",
    "        \"\"\"自我博弈获得训练数据\"\"\"\n",
    "        datas = self.game.self_play(self.MCTSPlayer, show=False)\n",
    "        for data in datas:\n",
    "            self.train_data.extend(self.augment(data))\n",
    "\n",
    "    def policy_updata(self, learning_rate):\n",
    "        \"\"\"策略升级\"\"\"\n",
    "        states, probs, winner = list(zip(*self.train_data))\n",
    "        states = np.array(states)\n",
    "        probs = np.array(probs)\n",
    "        winner = np.array(winner)\n",
    "        self.PolicyValueNet.fit(states, probs, winner, learning_rate, batch_size=self.batch_size)\n",
    "\n",
    "    def eva(self, model):\n",
    "        old_net = PolicyValueNet(self.board_length, initial_model=model)\n",
    "        old_MCTSplayer = MCTSPlayer(old_net.PolicyValueFunction)\n",
    "        win = []\n",
    "        for _ in range(5):\n",
    "            winner = self.game.play(self.MCTSPlayer, old_MCTSplayer, show=False)\n",
    "            win.append(winner == -1)\n",
    "        for _ in range(5):\n",
    "            winner = self.game.play(old_MCTSplayer, self.MCTSPlayer, show=False)\n",
    "            win.append(winner == 1)\n",
    "        return sum(win) / 10\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"评估当前模型是否有进步\"\"\"\n",
    "        if not self.initial_model:\n",
    "            return 1.0\n",
    "        old_net = PolicyValueNet(self.board_length, initial_model=self.initial_model)\n",
    "        old_MCTSplayer = MCTSPlayer(old_net.PolicyValueFunction)\n",
    "        win = []\n",
    "        for _ in range(5):\n",
    "            winner = self.game.play(self.MCTSPlayer, old_MCTSplayer, show=False)\n",
    "            win.append(winner == -1)\n",
    "        for _ in range(5):\n",
    "            winner = self.game.play(old_MCTSplayer, self.MCTSPlayer, show=False)\n",
    "            win.append(winner == 1)\n",
    "        return sum(win) / 10\n",
    "    \n",
    "    def train(self):\n",
    "        epoch = 0\n",
    "        try:\n",
    "            while True:\n",
    "                epoch += 1\n",
    "                for _ in range(self.game_rounds):\n",
    "                    self.collect_data()\n",
    "                lr = 0 if epoch < 400 else 1 if epoch < 600 else 2\n",
    "                logger.info(\"Policy update using %s, length %s\", self.train_data[:17:8], len(self.train_data))\n",
    "                self.policy_updata(learning_rate=self.learning_rate[lr])\n",
    "                self.train_data.clear()\n",
    "\n",
    "                if epoch % self.check_point == 0:\n",
    "                    ration = self.evaluate()\n",
    "                    logger.info(\"Policy evaluate: %s\", ration)\n",
    "                    if ration > 0.5:\n",
    "                        self.PolicyValueNet.save_model(self.initial_model)\n",
    "                        self.PolicyValueNet.save_model(f\"models/epoch_{epoch}\")\n",
    "        except KeyboardInterrupt:\n",
    "            self.PolicyValueNet.save_model('last_model')\n",
    "            logger.info(\"Done\")\n",
    "        finally:\n",
    "            self.PolicyValueNet.save_model('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3533e-4f3f-4ee9-aa99-dec5df984308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Train(\"error.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4c16d-5854-49f5-b0e8-92f50a19c53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
